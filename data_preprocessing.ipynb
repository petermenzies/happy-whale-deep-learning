{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d0f1642-7f7c-4f57-b68e-82fd4c6769a7",
   "metadata": {
    "id": "7d0f1642-7f7c-4f57-b68e-82fd4c6769a7",
    "tags": []
   },
   "source": [
    "### This nb creates a subset of the Happywhale training images and gets them all set up for use with `ImageDataGenerator`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00041c7e-0a47-4d44-8c6a-e2cd0de8114e",
   "metadata": {
    "id": "00041c7e-0a47-4d44-8c6a-e2cd0de8114e"
   },
   "source": [
    "If we decide to use the full set, it will be easy enough to adapt this code to structure all images for the data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bce4a38-95b1-4fe7-acfe-dcae7f6971ff",
   "metadata": {
    "id": "8bce4a38-95b1-4fe7-acfe-dcae7f6971ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 23:41:30.280320: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/R/4.0.5/lib/R/lib::/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/java-11-openjdk-amd64/lib/server\n",
      "2022-03-04 23:41:30.280351: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "redo = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11599759-61c5-480d-82c8-f6510de0b719",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11599759-61c5-480d-82c8-f6510de0b719",
    "outputId": "6581e2fe-def8-424d-89be-cc34a82ba7d4"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB == True:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86e3ca-220e-42fc-a234-a79506e150cb",
   "metadata": {
    "id": "4b86e3ca-220e-42fc-a234-a79506e150cb"
   },
   "source": [
    "#### Creating file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd91fc0d-d9db-4a3c-8362-1860411ec4da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd91fc0d-d9db-4a3c-8362-1860411ec4da",
    "outputId": "e0f61e2c-c5ca-47a6-8c79-ce7c552193c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['melon_headed_whale', 'humpback_whale', 'false_killer_whale',\n",
       "       'bottlenose_dolphin', 'beluga', 'minke_whale', 'fin_whale',\n",
       "       'blue_whale', 'gray_whale', 'southern_right_whale',\n",
       "       'common_dolphin', 'killer_whale', 'pilot_whale', 'dusky_dolphin',\n",
       "       'long_finned_pilot_whale', 'sei_whale', 'spinner_dolphin',\n",
       "       'cuviers_beaked_whale', 'spotted_dolphin', 'globis',\n",
       "       'brydes_whale', 'commersons_dolphin', 'white_sided_dolphin',\n",
       "       'short_finned_pilot_whale', 'rough_toothed_dolphin',\n",
       "       'pantropic_spotted_dolphin', 'pygmy_killer_whale',\n",
       "       'frasiers_dolphin'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if IN_COLAB == True:\n",
    "  wd = '/content/drive/Shareddrives/Whales-ML/'\n",
    "  df = pd.read_csv(wd + 'train.csv')\n",
    "else:\n",
    "  df = pd.read_csv('data/train.csv')\n",
    "\n",
    "# correcting mispelled species\n",
    "df['species'] = df['species'].replace('kiler_whale', 'killer_whale')\n",
    "df['species'] = df['species'].replace('bottlenose_dolpin', 'bottlenose_dolphin')\n",
    "image_names = df['image']\n",
    "sp_names = pd.unique(df['species'])\n",
    "sp_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7kev58Qw38gX",
   "metadata": {
    "id": "7kev58Qw38gX"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB == True:\n",
    "  train_subset_dir = '/content/drive/Shareddrives/Whales-ML/subset/train_subset/'\n",
    "  validation_subset_dir = '/content/drive/Shareddrives/Whales-ML/subset/validation_subset/'\n",
    "else:\n",
    "  train_subset_dir = 'data/subset/train_subset/'\n",
    "  validation_subset_dir = 'data/subset/validation_subset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f229f0b7-a94b-481b-9b9c-88c3c5a00266",
   "metadata": {
    "id": "f229f0b7-a94b-481b-9b9c-88c3c5a00266"
   },
   "outputs": [],
   "source": [
    "if redo == True:\n",
    "\n",
    "    os.mkdir(train_subset_dir)\n",
    "    os.mkdir(validation_subset_dir)\n",
    "\n",
    "    for i in range(0, len(sp_names)):\n",
    "        os.mkdir(train_subset_dir + sp_names[i])\n",
    "        os.mkdir(validation_subset_dir + sp_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16557f53-feeb-42b4-ad8e-3ef9c0e1beeb",
   "metadata": {
    "id": "16557f53-feeb-42b4-ad8e-3ef9c0e1beeb"
   },
   "source": [
    "#### Copying files into new directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f170b408-4a0f-4f9b-b061-36b4822cfe00",
   "metadata": {
    "id": "f170b408-4a0f-4f9b-b061-36b4822cfe00"
   },
   "outputs": [],
   "source": [
    "if redo == True:\n",
    "\n",
    "    train_dir = 'data/train_images/'\n",
    "    train_nsamples = len(os.listdir(train_dir))\n",
    "\n",
    "    # randomly selecting 4000 training images and 1000 validation images\n",
    "    seed_value = 71993\n",
    "    random.seed(seed_value)\n",
    "    sample_indices = random.sample(range(0, train_nsamples), 5000)\n",
    "    train_indices = sample_indices[0:4000]\n",
    "    validation_indices = sample_indices[4000:5000]\n",
    "\n",
    "    train_subset_df = df.filter(items = train_indices, axis=0)\n",
    "    validation_subset_df = df.filter(items = validation_indices, axis=0)\n",
    "    \n",
    "    for sp in sp_names:\n",
    "        sp_df = train_subset_df[train_subset_df['species']==sp]\n",
    "\n",
    "        for i in range(0, len(sp_df)):\n",
    "            src = train_dir + sp_df['image'].iloc[i]\n",
    "            dst = train_subset_dir + sp + '/' + sp_df['image'].iloc[i]\n",
    "            shutil.copyfile(src, dst)\n",
    "\n",
    "        sp_df = validation_subset_df[validation_subset_df['species']==sp]\n",
    "\n",
    "        for i in range(0, len(sp_df)):\n",
    "            src = train_dir + sp_df['image'].iloc[i]\n",
    "            dst = validation_subset_dir + sp + '/' + sp_df['image'].iloc[i]\n",
    "            shutil.copyfile(src, dst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ed63d-bdf1-4bb2-8991-149360f48d2e",
   "metadata": {
    "id": "8c9ed63d-bdf1-4bb2-8991-149360f48d2e"
   },
   "source": [
    "#### Looking at the representation of each species in the subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a72757a-f355-4b3f-bbf7-594be9f61f94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a72757a-f355-4b3f-bbf7-594be9f61f94",
    "outputId": "6fdd3d95-20f3-46b6-89f5-527691c081aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melon_headed_whale: 125\n",
      "humpback_whale: 590\n",
      "false_killer_whale: 246\n",
      "bottlenose_dolphin: 828\n",
      "beluga: 606\n",
      "minke_whale: 121\n",
      "fin_whale: 115\n",
      "blue_whale: 357\n",
      "gray_whale: 79\n",
      "southern_right_whale: 73\n",
      "common_dolphin: 28\n",
      "killer_whale: 212\n",
      "pilot_whale: 18\n",
      "dusky_dolphin: 256\n",
      "long_finned_pilot_whale: 15\n",
      "sei_whale: 37\n",
      "spinner_dolphin: 135\n",
      "cuviers_beaked_whale: 28\n",
      "spotted_dolphin: 35\n",
      "globis: 8\n",
      "brydes_whale: 9\n",
      "commersons_dolphin: 12\n",
      "white_sided_dolphin: 16\n",
      "short_finned_pilot_whale: 30\n",
      "rough_toothed_dolphin: 5\n",
      "pantropic_spotted_dolphin: 8\n",
      "pygmy_killer_whale: 5\n",
      "frasiers_dolphin: 3\n"
     ]
    }
   ],
   "source": [
    "for sp in sp_names:\n",
    "    sp_dir = train_subset_dir + sp\n",
    "    nsamples = len(os.listdir(sp_dir))\n",
    "    print(f'{sp}: {nsamples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7jM2Z2ikyjny",
   "metadata": {
    "id": "7jM2Z2ikyjny"
   },
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "O4Zf6PAfyV-O",
   "metadata": {
    "id": "O4Zf6PAfyV-O"
   },
   "outputs": [],
   "source": [
    "def make_path(dataset, file_name):\n",
    "  path = locals()[dataset + '_subset_dir'] + '/' + file_name\n",
    "  return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "r8-w1nJy4q2m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8-w1nJy4q2m",
    "outputId": "6e534094-b705-498a-96a8-a32124da0780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 28 classes.\n",
      "Found 1000 images belonging to 28 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 100\n",
    "# train_steps = 4000 / batch_size\n",
    "# validation_steps = 1000 / batch_size\n",
    "train_steps = 20\n",
    "validation_steps = 10\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_subset_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_subset_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "iTtivIap5Cj7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iTtivIap5Cj7",
    "outputId": "547a0d73-9d39-4013-d985-ee5efbeb0847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (100, 150, 150, 3)\n",
      "labels batch shape: (100, 28)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "GiiyNErG9CGA",
   "metadata": {
    "id": "GiiyNErG9CGA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 23:44:51.828447: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/R/4.0.5/lib/R/lib::/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/java-11-openjdk-amd64/lib/server\n",
      "2022-03-04 23:44:51.828479: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-04 23:44:51.828501: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (taylor): /proc/driver/nvidia/version does not exist\n",
      "2022-03-04 23:44:51.828772: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(28, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9YuI8Aoo8d9A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "9YuI8Aoo8d9A",
    "outputId": "7cf1ac1b-bd82-46cf-cb03-0fbf45255dc8"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=train_steps,\n",
    "      epochs=10,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ixAQlxRM-WNH",
   "metadata": {
    "id": "ixAQlxRM-WNH"
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "data_preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.7",
   "language": "python",
   "name": "py3.7.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
