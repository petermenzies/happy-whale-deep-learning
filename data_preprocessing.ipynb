{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7d0f1642-7f7c-4f57-b68e-82fd4c6769a7",
      "metadata": {
        "tags": [],
        "id": "7d0f1642-7f7c-4f57-b68e-82fd4c6769a7"
      },
      "source": [
        "### This nb creates a subset of the Happywhale training images and gets them all set up for use with `ImageDataGenerator`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00041c7e-0a47-4d44-8c6a-e2cd0de8114e",
      "metadata": {
        "id": "00041c7e-0a47-4d44-8c6a-e2cd0de8114e"
      },
      "source": [
        "If we decide to use the full set, it will be easy enough to adapt this code to structure all images for the data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "8bce4a38-95b1-4fe7-acfe-dcae7f6971ff",
      "metadata": {
        "id": "8bce4a38-95b1-4fe7-acfe-dcae7f6971ff"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "redo = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "11599759-61c5-480d-82c8-f6510de0b719",
      "metadata": {
        "id": "11599759-61c5-480d-82c8-f6510de0b719",
        "outputId": "86cdeaf7-047c-4780-bbda-0e7027bfb024",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB == True:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b86e3ca-220e-42fc-a234-a79506e150cb",
      "metadata": {
        "id": "4b86e3ca-220e-42fc-a234-a79506e150cb"
      },
      "source": [
        "#### Creating file structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "fd91fc0d-d9db-4a3c-8362-1860411ec4da",
      "metadata": {
        "id": "fd91fc0d-d9db-4a3c-8362-1860411ec4da",
        "outputId": "58cecb6c-6fd5-4955-d2a7-7e8ff23f940f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['melon_headed_whale', 'humpback_whale', 'false_killer_whale',\n",
              "       'bottlenose_dolphin', 'beluga', 'minke_whale', 'fin_whale',\n",
              "       'blue_whale', 'gray_whale', 'southern_right_whale',\n",
              "       'common_dolphin', 'killer_whale', 'pilot_whale', 'dusky_dolphin',\n",
              "       'long_finned_pilot_whale', 'sei_whale', 'spinner_dolphin',\n",
              "       'cuviers_beaked_whale', 'spotted_dolphin', 'globis',\n",
              "       'brydes_whale', 'commersons_dolphin', 'white_sided_dolphin',\n",
              "       'short_finned_pilot_whale', 'rough_toothed_dolphin',\n",
              "       'pantropic_spotted_dolphin', 'pygmy_killer_whale',\n",
              "       'frasiers_dolphin'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "if IN_COLAB == True:\n",
        "  wd = '/content/drive/Shareddrives/Whales-ML/'\n",
        "  df = pd.read_csv(wd + 'train.csv')\n",
        "else:\n",
        "  df = pd.read_csv('data/train.csv')\n",
        "\n",
        "# correcting mispelled species\n",
        "df['species'] = df['species'].replace('kiler_whale', 'killer_whale')\n",
        "df['species'] = df['species'].replace('bottlenose_dolpin', 'bottlenose_dolphin')\n",
        "image_names = df['image']\n",
        "sp_names = pd.unique(df['species'])\n",
        "sp_names"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if IN_COLAB == True:\n",
        "  train_subset_dir = '/content/drive/Shareddrives/Whales-ML/subset/train_subset/'\n",
        "  validation_subset_dir = '/content/drive/Shareddrives/Whales-ML/subset/validation_subset/'\n",
        "else:\n",
        "  train_subset_dir = 'data/subset/train_subset/'\n",
        "  validation_subset_dir = 'data/subset/validation_subset/'"
      ],
      "metadata": {
        "id": "7kev58Qw38gX"
      },
      "id": "7kev58Qw38gX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "f229f0b7-a94b-481b-9b9c-88c3c5a00266",
      "metadata": {
        "id": "f229f0b7-a94b-481b-9b9c-88c3c5a00266"
      },
      "outputs": [],
      "source": [
        "if redo == True:\n",
        "\n",
        "    os.mkdir(train_subset_dir)\n",
        "    os.mkdir(validation_subset_dir)\n",
        "\n",
        "    for i in range(0, len(sp_names)):\n",
        "        os.mkdir(train_subset_dir + sp_names[i])\n",
        "        os.mkdir(validation_subset_dir + sp_names[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16557f53-feeb-42b4-ad8e-3ef9c0e1beeb",
      "metadata": {
        "id": "16557f53-feeb-42b4-ad8e-3ef9c0e1beeb"
      },
      "source": [
        "#### Copying files into new directory structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "f170b408-4a0f-4f9b-b061-36b4822cfe00",
      "metadata": {
        "id": "f170b408-4a0f-4f9b-b061-36b4822cfe00"
      },
      "outputs": [],
      "source": [
        "if redo == True:\n",
        "\n",
        "    train_dir = 'data/train_images/'\n",
        "    train_nsamples = len(os.listdir(train_dir))\n",
        "\n",
        "    # randomly selecting 4000 training images and 1000 validation images\n",
        "    seed_value = 71993\n",
        "    random.seed(seed_value)\n",
        "    sample_indices = random.sample(range(0, train_nsamples), 5000)\n",
        "    train_indices = sample_indices[0:4000]\n",
        "    validation_indices = sample_indices[4000:5000]\n",
        "\n",
        "    train_subset_df = df.filter(items = train_indices, axis=0)\n",
        "    validation_subset_df = df.filter(items = validation_indices, axis=0)\n",
        "    \n",
        "    for sp in sp_names:\n",
        "        sp_df = train_subset_df[train_subset_df['species']==sp]\n",
        "\n",
        "        for i in range(0, len(sp_df)):\n",
        "            src = train_dir + sp_df['image'].iloc[i]\n",
        "            dst = train_subset_dir + sp + '/' + sp_df['image'].iloc[i]\n",
        "            shutil.copyfile(src, dst)\n",
        "\n",
        "        sp_df = validation_subset_df[validation_subset_df['species']==sp]\n",
        "\n",
        "        for i in range(0, len(sp_df)):\n",
        "            src = train_dir + sp_df['image'].iloc[i]\n",
        "            dst = validation_subset_dir + sp + '/' + sp_df['image'].iloc[i]\n",
        "            shutil.copyfile(src, dst)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c9ed63d-bdf1-4bb2-8991-149360f48d2e",
      "metadata": {
        "id": "8c9ed63d-bdf1-4bb2-8991-149360f48d2e"
      },
      "source": [
        "#### Looking at the representation of each species in the subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "1a72757a-f355-4b3f-bbf7-594be9f61f94",
      "metadata": {
        "id": "1a72757a-f355-4b3f-bbf7-594be9f61f94",
        "outputId": "7c4fef0f-3296-4206-80b4-8412588e0714",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "melon_headed_whale: 125\n",
            "humpback_whale: 590\n",
            "false_killer_whale: 246\n",
            "bottlenose_dolphin: 738\n",
            "beluga: 606\n",
            "minke_whale: 121\n",
            "fin_whale: 115\n",
            "blue_whale: 357\n",
            "gray_whale: 79\n",
            "southern_right_whale: 73\n",
            "common_dolphin: 28\n",
            "killer_whale: 127\n",
            "pilot_whale: 18\n",
            "dusky_dolphin: 256\n",
            "long_finned_pilot_whale: 15\n",
            "sei_whale: 37\n",
            "spinner_dolphin: 135\n",
            "cuviers_beaked_whale: 28\n",
            "spotted_dolphin: 35\n",
            "globis: 8\n",
            "brydes_whale: 9\n",
            "commersons_dolphin: 12\n",
            "white_sided_dolphin: 16\n",
            "short_finned_pilot_whale: 30\n",
            "rough_toothed_dolphin: 5\n",
            "pantropic_spotted_dolphin: 8\n",
            "pygmy_killer_whale: 5\n",
            "frasiers_dolphin: 3\n"
          ]
        }
      ],
      "source": [
        "for sp in sp_names:\n",
        "    sp_dir = train_subset_dir + sp\n",
        "    nsamples = len(os.listdir(sp_dir))\n",
        "    print(f'{sp}: {nsamples}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data generator"
      ],
      "metadata": {
        "id": "7jM2Z2ikyjny"
      },
      "id": "7jM2Z2ikyjny"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_path(dataset, file_name):\n",
        "  path = locals()[dataset + '_subset_dir'] + '/' + file_name\n",
        "  return path"
      ],
      "metadata": {
        "id": "O4Zf6PAfyV-O"
      },
      "id": "O4Zf6PAfyV-O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size = 50\n",
        "train_steps = 4000 / batch_size\n",
        "validation_steps = 1000 / batch_size\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_subset_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_subset_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')"
      ],
      "metadata": {
        "id": "r8-w1nJy4q2m",
        "outputId": "d3894173-a627-4542-f2af-55601f2df3ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "r8-w1nJy4q2m",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4000 images belonging to 28 classes.\n",
            "Found 1000 images belonging to 28 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data_batch, labels_batch in train_generator:\n",
        "    print('data batch shape:', data_batch.shape)\n",
        "    print('labels batch shape:', labels_batch.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "iTtivIap5Cj7",
        "outputId": "5972a360-2429-4815-fd4d-30723998de30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "iTtivIap5Cj7",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data batch shape: (50, 150, 150, 3)\n",
            "labels batch shape: (50,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
        "              metrics=['acc'])"
      ],
      "metadata": {
        "id": "GiiyNErG9CGA"
      },
      "id": "GiiyNErG9CGA",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=train_steps,\n",
        "      epochs=10,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=validation_steps)"
      ],
      "metadata": {
        "id": "9YuI8Aoo8d9A"
      },
      "id": "9YuI8Aoo8d9A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ixAQlxRM-WNH"
      },
      "id": "ixAQlxRM-WNH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.7",
      "language": "python",
      "name": "py3.7.7"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "data_preprocessing.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}