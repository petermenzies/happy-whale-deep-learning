{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7d0f1642-7f7c-4f57-b68e-82fd4c6769a7",
      "metadata": {
        "tags": [],
        "id": "7d0f1642-7f7c-4f57-b68e-82fd4c6769a7"
      },
      "source": [
        "### This nb creates a subset of the Happywhale training images and gets them all set up for use with `ImageDataGenerator`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00041c7e-0a47-4d44-8c6a-e2cd0de8114e",
      "metadata": {
        "id": "00041c7e-0a47-4d44-8c6a-e2cd0de8114e"
      },
      "source": [
        "If we decide to use the full set, it will be easy enough to adapt this code to structure all images for the data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8bce4a38-95b1-4fe7-acfe-dcae7f6971ff",
      "metadata": {
        "id": "8bce4a38-95b1-4fe7-acfe-dcae7f6971ff"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "redo = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "11599759-61c5-480d-82c8-f6510de0b719",
      "metadata": {
        "id": "11599759-61c5-480d-82c8-f6510de0b719",
        "outputId": "6581e2fe-def8-424d-89be-cc34a82ba7d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB == True:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b86e3ca-220e-42fc-a234-a79506e150cb",
      "metadata": {
        "id": "4b86e3ca-220e-42fc-a234-a79506e150cb"
      },
      "source": [
        "#### Creating file structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fd91fc0d-d9db-4a3c-8362-1860411ec4da",
      "metadata": {
        "id": "fd91fc0d-d9db-4a3c-8362-1860411ec4da",
        "outputId": "e0f61e2c-c5ca-47a6-8c79-ce7c552193c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['melon_headed_whale', 'humpback_whale', 'false_killer_whale',\n",
              "       'bottlenose_dolphin', 'beluga', 'minke_whale', 'fin_whale',\n",
              "       'blue_whale', 'gray_whale', 'southern_right_whale',\n",
              "       'common_dolphin', 'killer_whale', 'pilot_whale', 'dusky_dolphin',\n",
              "       'long_finned_pilot_whale', 'sei_whale', 'spinner_dolphin',\n",
              "       'cuviers_beaked_whale', 'spotted_dolphin', 'globis',\n",
              "       'brydes_whale', 'commersons_dolphin', 'white_sided_dolphin',\n",
              "       'short_finned_pilot_whale', 'rough_toothed_dolphin',\n",
              "       'pantropic_spotted_dolphin', 'pygmy_killer_whale',\n",
              "       'frasiers_dolphin'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "if IN_COLAB == True:\n",
        "  wd = '/content/drive/Shareddrives/Whales-ML/'\n",
        "  df = pd.read_csv(wd + 'train.csv')\n",
        "else:\n",
        "  df = pd.read_csv('data/train.csv')\n",
        "\n",
        "# correcting mispelled species\n",
        "df['species'] = df['species'].replace('kiler_whale', 'killer_whale')\n",
        "df['species'] = df['species'].replace('bottlenose_dolpin', 'bottlenose_dolphin')\n",
        "image_names = df['image']\n",
        "sp_names = pd.unique(df['species'])\n",
        "sp_names"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if IN_COLAB == True:\n",
        "  train_subset_dir = '/content/drive/Shareddrives/Whales-ML/subset/train_subset/'\n",
        "  validation_subset_dir = '/content/drive/Shareddrives/Whales-ML/subset/validation_subset/'\n",
        "else:\n",
        "  train_subset_dir = 'data/subset/train_subset/'\n",
        "  validation_subset_dir = 'data/subset/validation_subset/'"
      ],
      "metadata": {
        "id": "7kev58Qw38gX"
      },
      "id": "7kev58Qw38gX",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f229f0b7-a94b-481b-9b9c-88c3c5a00266",
      "metadata": {
        "id": "f229f0b7-a94b-481b-9b9c-88c3c5a00266"
      },
      "outputs": [],
      "source": [
        "if redo == True:\n",
        "\n",
        "    os.mkdir(train_subset_dir)\n",
        "    os.mkdir(validation_subset_dir)\n",
        "\n",
        "    for i in range(0, len(sp_names)):\n",
        "        os.mkdir(train_subset_dir + sp_names[i])\n",
        "        os.mkdir(validation_subset_dir + sp_names[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16557f53-feeb-42b4-ad8e-3ef9c0e1beeb",
      "metadata": {
        "id": "16557f53-feeb-42b4-ad8e-3ef9c0e1beeb"
      },
      "source": [
        "#### Copying files into new directory structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f170b408-4a0f-4f9b-b061-36b4822cfe00",
      "metadata": {
        "id": "f170b408-4a0f-4f9b-b061-36b4822cfe00"
      },
      "outputs": [],
      "source": [
        "if redo == True:\n",
        "\n",
        "    train_dir = 'data/train_images/'\n",
        "    train_nsamples = len(os.listdir(train_dir))\n",
        "\n",
        "    # randomly selecting 4000 training images and 1000 validation images\n",
        "    seed_value = 71993\n",
        "    random.seed(seed_value)\n",
        "    sample_indices = random.sample(range(0, train_nsamples), 5000)\n",
        "    train_indices = sample_indices[0:4000]\n",
        "    validation_indices = sample_indices[4000:5000]\n",
        "\n",
        "    train_subset_df = df.filter(items = train_indices, axis=0)\n",
        "    validation_subset_df = df.filter(items = validation_indices, axis=0)\n",
        "    \n",
        "    for sp in sp_names:\n",
        "        sp_df = train_subset_df[train_subset_df['species']==sp]\n",
        "\n",
        "        for i in range(0, len(sp_df)):\n",
        "            src = train_dir + sp_df['image'].iloc[i]\n",
        "            dst = train_subset_dir + sp + '/' + sp_df['image'].iloc[i]\n",
        "            shutil.copyfile(src, dst)\n",
        "\n",
        "        sp_df = validation_subset_df[validation_subset_df['species']==sp]\n",
        "\n",
        "        for i in range(0, len(sp_df)):\n",
        "            src = train_dir + sp_df['image'].iloc[i]\n",
        "            dst = validation_subset_dir + sp + '/' + sp_df['image'].iloc[i]\n",
        "            shutil.copyfile(src, dst)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c9ed63d-bdf1-4bb2-8991-149360f48d2e",
      "metadata": {
        "id": "8c9ed63d-bdf1-4bb2-8991-149360f48d2e"
      },
      "source": [
        "#### Looking at the representation of each species in the subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1a72757a-f355-4b3f-bbf7-594be9f61f94",
      "metadata": {
        "id": "1a72757a-f355-4b3f-bbf7-594be9f61f94",
        "outputId": "6fdd3d95-20f3-46b6-89f5-527691c081aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "melon_headed_whale: 125\n",
            "humpback_whale: 590\n",
            "false_killer_whale: 246\n",
            "bottlenose_dolphin: 828\n",
            "beluga: 606\n",
            "minke_whale: 121\n",
            "fin_whale: 115\n",
            "blue_whale: 357\n",
            "gray_whale: 79\n",
            "southern_right_whale: 73\n",
            "common_dolphin: 28\n",
            "killer_whale: 212\n",
            "pilot_whale: 18\n",
            "dusky_dolphin: 256\n",
            "long_finned_pilot_whale: 15\n",
            "sei_whale: 37\n",
            "spinner_dolphin: 135\n",
            "cuviers_beaked_whale: 28\n",
            "spotted_dolphin: 35\n",
            "globis: 8\n",
            "brydes_whale: 9\n",
            "commersons_dolphin: 12\n",
            "white_sided_dolphin: 16\n",
            "short_finned_pilot_whale: 30\n",
            "rough_toothed_dolphin: 5\n",
            "pantropic_spotted_dolphin: 8\n",
            "pygmy_killer_whale: 5\n",
            "frasiers_dolphin: 3\n"
          ]
        }
      ],
      "source": [
        "for sp in sp_names:\n",
        "    sp_dir = train_subset_dir + sp\n",
        "    nsamples = len(os.listdir(sp_dir))\n",
        "    print(f'{sp}: {nsamples}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data generator"
      ],
      "metadata": {
        "id": "7jM2Z2ikyjny"
      },
      "id": "7jM2Z2ikyjny"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_path(dataset, file_name):\n",
        "  path = locals()[dataset + '_subset_dir'] + '/' + file_name\n",
        "  return path"
      ],
      "metadata": {
        "id": "O4Zf6PAfyV-O"
      },
      "id": "O4Zf6PAfyV-O",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size = 50\n",
        "train_steps = 4000 / batch_size\n",
        "validation_steps = 1000 / batch_size\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_subset_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_subset_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')"
      ],
      "metadata": {
        "id": "r8-w1nJy4q2m",
        "outputId": "6e534094-b705-498a-96a8-a32124da0780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "r8-w1nJy4q2m",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4000 images belonging to 28 classes.\n",
            "Found 1000 images belonging to 28 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data_batch, labels_batch in train_generator:\n",
        "    print('data batch shape:', data_batch.shape)\n",
        "    print('labels batch shape:', labels_batch.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "iTtivIap5Cj7",
        "outputId": "547a0d73-9d39-4013-d985-ee5efbeb0847",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "iTtivIap5Cj7",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data batch shape: (50, 150, 150, 3)\n",
            "labels batch shape: (50,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
        "              metrics=['acc'])"
      ],
      "metadata": {
        "id": "GiiyNErG9CGA"
      },
      "id": "GiiyNErG9CGA",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=train_steps,\n",
        "      epochs=3,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=validation_steps)"
      ],
      "metadata": {
        "id": "9YuI8Aoo8d9A",
        "outputId": "7cf1ac1b-bd82-46cf-cb03-0fbf45255dc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "id": "9YuI8Aoo8d9A",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            " 1/80 [..............................] - ETA: 39:52 - loss: 0.0000e+00 - acc: 0.1000"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0aa6d8eaf518>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       validation_steps=validation_steps)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ixAQlxRM-WNH"
      },
      "id": "ixAQlxRM-WNH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.7",
      "language": "python",
      "name": "py3.7.7"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "data_preprocessing.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}